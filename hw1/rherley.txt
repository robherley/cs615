Robert Herley <rherley>
HW1 - EC2 Setup & Creating Instance

Setup
=====

1.  Configuring the environment.
    Since I am using the linux lab, the AWS CLI is already installed. To 
    configure my credentials, I ran "aws configure" and then my AWS Access 
    keys for my root account, as well as set the region (us-east-1), and the
    preferred output type. This created two files, "~/.aws/config" and 
    "~/.aws/credentials".
2.  Creating the IAM Group & User.
    In the getting started guide, it is recommended to use AWS Identity and 
    Access Management, instead of directly using AWS credentials of the root
    account. First I created a group called admins:
        "aws iam create-group --group-name admins"
    Then, I attached a policy that lets any user in the group to access to any
    action or resource:
        "aws iam attach-group-policy --group-name admins \
            --policy-arn arn:aws:iam::aws:policy/AdministratorAccess"
    Now, I can create a user:
        "aws iam create-user --user-name rob"
    Then attach the user to the administrator group:
        "aws iam add-user-to-group --group-name admins --user-name rob"
    Get the access keys to a user and append them to a file:
        "aws iam create-access-key --user-name rob > keys.json"
    Finally, I delete the old credentials files (~/.aws/*) with the root access 
    keys, and run the configuration again with the new IAM user keys.


Creating The Instance
=====================

1.  Create a key pair in order to connect to a EC2 instance:
        "aws ec2 create-key-pair --key-name aws_kp --query 'KeyMaterial' \
            --output text > ~/.aws/aws_kp.pem"
    Change permissions so only I can read the key pair file:
        "chmod 400 aws_kp.pem"
2.  Find our default security group (aws ec2 describe-security-groups), and 
    restrict the inbound SSH traffic to Stevens IPs, for safety.
        "aws ec2 authorize-security-group-ingress \
            --group-id <sg id>                    \
            --protocol tcp                        \
            --port 22                             \
            --cidr 155.246.0.0/16"
3.  Create the NetBSD t1.micro instance:
        "aws ec2 run-instances              \
            --image-id ami-bc8fc8d6         \
            --count 1                       \
            --instance-type t1.micro        \
            --key-name aws_kp               \
            --security-group-ids <sg id>    \
            --subnet-id <subnet id>"
        (Subnet ID can be found from "aws ec2 describe-subnets")
4.  Connect using SSH:
        "ssh -i ~/.aws/aws_kp.pem root@<public instance dns>"


Command Outputs
===============

"uname -a"
----------
NetBSD ip-172-31-1-182.ec2.internal 7.0 NetBSD 7.0 (XEN3_DOMU.201509250726Z) amd64


"whoami"
--------
root


"date"
------
Sun Feb  3 03:18:42 UTC 2019


"w"
---
 3:18AM  up 9 mins, 1 user, load averages: 0.00, 0.04, 0.01
USER     TTY     FROM                          LOGIN@  IDLE WHAT
root     pts/0   shell.srcit.stevens-tech.edu  3:17AM     0 w


"ifconfig -a"
-------------
xennet0: flags=8863<UP,BROADCAST,NOTRAILERS,RUNNING,SIMPLEX,MULTICAST> mtu 1500
	capabilities=2800<TCP4CSUM_Tx,UDP4CSUM_Tx>
	enabled=0
	address: 02:b0:23:d7:51:bc
	inet 172.31.1.182 netmask 0xfffff000 broadcast 172.31.15.255
	inet6 fe80::8c72:b8a5:4fc9:4d00%xennet0 prefixlen 64 scopeid 0x1
lo0: flags=8049<UP,LOOPBACK,RUNNING,MULTICAST> mtu 33648
	inet 127.0.0.1 netmask 0xff000000
	inet6 ::1 prefixlen 128
	inet6 fe80::1%lo0 prefixlen 64 scopeid 0x2


"netstat -na"
-------------
Active Internet connections (including servers)
Proto Recv-Q Send-Q  Local Address          Foreign Address        State
tcp        0     36  172.31.1.182.22        155.246.89.72.46754    ESTABLISHED
tcp        0      0  *.22                   *.*                    LISTEN
udp        0      0  *.68                   *.*
Active Internet6 connections (including servers)
Proto Recv-Q Send-Q  Local Address          Foreign Address        (state)
tcp6       0      0  *.22                   *.*                    LISTEN
Active UNIX domain sockets
Address  Type   Recv-Q Send-Q    Inode     Conn     Refs  Nextref Addr
ffffa00001f998d0 stream      0      0        0 ffffa00001f99940        0        0
ffffa00001f99940 stream      0      0        0 ffffa00001f998d0        0        0
ffffa00001f999b0 stream      0      0 ffffa00001f902a0        0        0        0 private/scache
ffffa00001f99a20 stream      0      0        0 ffffa00001f99a90        0        0
ffffa00001f99a90 stream      0      0        0 ffffa00001f99a20        0        0
ffffa00001f99b00 stream      0      0 ffffa00001f903c8        0        0        0 private/anvil
ffffa00001f99b70 stream      0      0        0 ffffa00001f99be0        0        0
ffffa00001f99be0 stream      0      0        0 ffffa00001f99b70        0        0
ffffa00001f99c50 stream      0      0 ffffa00001f904f0        0        0        0 private/lmtp
ffffa00001f99cc0 stream      0      0        0 ffffa00001f99d30        0        0
ffffa00001f99d30 stream      0      0        0 ffffa00001f99cc0        0        0
ffffa00001f99da0 stream      0      0 ffffa00001f90618        0        0        0 private/virtual
ffffa00001f99e10 stream      0      0        0 ffffa00001f99e80        0        0
ffffa00001f99e80 stream      0      0        0 ffffa00001f99e10        0        0
ffffa00001f99ef0 stream      0      0 ffffa00001f90740        0        0        0 private/local
ffffa00001f99f60 stream      0      0        0 ffffa00001f85008        0        0
ffffa00001f85008 stream      0      0        0 ffffa00001f99f60        0        0
ffffa00001f85078 stream      0      0 ffffa00001f90868        0        0        0 private/discard
ffffa00001f850e8 stream      0      0        0 ffffa00001f85158        0        0
ffffa00001f85158 stream      0      0        0 ffffa00001f850e8        0        0
ffffa00001f851c8 stream      0      0 ffffa00001f90990        0        0        0 private/retry
ffffa00001f85238 stream      0      0        0 ffffa00001f852a8        0        0
ffffa00001f852a8 stream      0      0        0 ffffa00001f85238        0        0
ffffa00001f85318 stream      0      0 ffffa00001f90ab8        0        0        0 private/error
ffffa00001f85388 stream      0      0        0 ffffa00001f853f8        0        0
ffffa00001f853f8 stream      0      0        0 ffffa00001f85388        0        0
ffffa00001f85468 stream      0      0 ffffa00001f90be0        0        0        0 public/showq
ffffa00001f854d8 stream      0      0        0 ffffa00001f85548        0        0
ffffa00001f85548 stream      0      0        0 ffffa00001f854d8        0        0
ffffa00001f855b8 stream      0      0 ffffa00001f90d08        0        0        0 private/relay
ffffa00001f85628 stream      0      0        0 ffffa00001f85698        0        0
ffffa00001f85698 stream      0      0        0 ffffa00001f85628        0        0
ffffa00001f85708 stream      0      0 ffffa00001f90e30        0        0        0 private/smtp
ffffa00001f85778 stream      0      0        0 ffffa00001f857e8        0        0
ffffa00001f857e8 stream      0      0        0 ffffa00001f85778        0        0
ffffa00001f85858 stream      0      0 ffffa00001f71048        0        0        0 private/proxywrite
ffffa00001f858c8 stream      0      0        0 ffffa00001f85938        0        0
ffffa00001f85938 stream      0      0        0 ffffa00001f858c8        0        0
ffffa00001f859a8 stream      0      0 ffffa00001f71170        0        0        0 private/proxymap
ffffa00001f85a18 stream      0      0        0 ffffa00001f85a88        0        0
ffffa00001f85a88 stream      0      0        0 ffffa00001f85a18        0        0
ffffa00001f85af8 stream      0      0 ffffa00001f71298        0        0        0 public/flush
ffffa00001f85b68 stream      0      0        0 ffffa00001f85bd8        0        0
ffffa00001f85bd8 stream      0      0        0 ffffa00001f85b68        0        0
ffffa00001f85c48 stream      0      0 ffffa00001f713c0        0        0        0 private/verify
ffffa00001f85cb8 stream      0      0        0 ffffa00001f85d28        0        0
ffffa00001f85d28 stream      0      0        0 ffffa00001f85cb8        0        0
ffffa00001f85d98 stream      0      0 ffffa00001f714e8        0        0        0 private/trace
ffffa00001f85e08 stream      0      0        0 ffffa00001f85e78        0        0
ffffa00001f85e78 stream      0      0        0 ffffa00001f85e08        0        0
ffffa00001f85ee8 stream      0      0 ffffa00001f71610        0        0        0 private/defer
ffffa00001f85f58 stream      0      0        0 ffffa000010cc000        0        0
ffffa000010cc070 stream      0      0 ffffa00001f71738        0        0        0 private/bounce
ffffa000010cc0e0 stream      0      0        0 ffffa000010cc150        0        0
ffffa000010cc150 stream      0      0        0 ffffa000010cc0e0        0        0
ffffa000010cc1c0 stream      0      0 ffffa00001f71860        0        0        0 private/rewrite
ffffa000010cc230 stream      0      0        0 ffffa000010cc2a0        0        0
ffffa000010cc2a0 stream      0      0        0 ffffa000010cc230        0        0
ffffa000010cc310 stream      0      0 ffffa00001f71988        0        0        0 private/tlsmgr
ffffa000010cc460 stream      0      0        0 ffffa000010cc4d0        0        0
ffffa000010cc4d0 stream      0      0        0 ffffa000010cc460        0        0
ffffa000010cc540 stream      0      0 ffffa00001f71ab0        0        0        0 public/qmgr
ffffa000010cc5b0 stream      0      0        0 ffffa000010cc620        0        0
ffffa000010cc620 stream      0      0        0 ffffa000010cc5b0        0        0
ffffa000010cc690 stream      0      0 ffffa00001f71bd8        0        0        0 public/cleanup
ffffa000010cc7e0 stream      0      0        0 ffffa000010cc850        0        0
ffffa000010cc850 stream      0      0        0 ffffa000010cc7e0        0        0
ffffa000010cc000 stream      0      0        0 ffffa00001f85f58        0        0
ffffa000010cc8c0 stream      0      0 ffffa00001f71d00        0        0        0 public/pickup
ffffa000010cc380 dgram       0      0        0 ffffa000010ccaf0        0 ffffa00001f997f0 -> /var/run/log
ffffa00001f99550 dgram       0      0        0        0        0        0
ffffa00001f99860 dgram       0      0        0 ffffa000010ccaf0        0 ffffa000010cc930 -> /var/run/log
ffffa000010cc930 dgram       0      0        0 ffffa000010ccaf0        0 ffffa000010cc9a0 -> /var/run/log
ffffa00001f997f0 dgram       0      0        0 ffffa000010ccaf0        0 ffffa00001f99860 -> /var/run/log
ffffa000010cc9a0 dgram       0      0        0 ffffa000010ccaf0        0 ffffa000010cca80 -> /var/run/log
ffffa000010cca80 dgram       0      0        0 ffffa000010ccaf0        0        0 -> /var/run/log
ffffa000010cca10 dgram       0      0        0        0        0        0
ffffa000010ccaf0 dgram       0      0 ffffa000019c36f0        0 ffffa000010cc380        0 /var/run/log


fdisk (partition table)
-----------------------
sk: primary partition table invalid, no magic in sector 0
fdisk: Cannot determine the number of heads
Disk: /dev/rxbd1d
NetBSD disklabel disk geometry:
cylinders: 1024, heads: 1, sectors/track: 2048 (2048 sectors/cylinder)
total sectors: 2097152, bytes/sector: 512

BIOS disk geometry:
cylinders: 130, heads: 255, sectors/track: 63 (16065 sectors/cylinder)
total sectors: 2097152

Partitions aligned to 16065 sector boundaries, offset 63

Partition table:
0: <UNUSED>
1: <UNUSED>
2: <UNUSED>
3: <UNUSED>
Bootselector disabled.
No active partition.
Drive serial number: 0 (0x00000000)


mount (mounted fs)
------------------
/dev/xbd1a on / type ffs (local)
/dev/xbd0a on /grub type ext2fs (local)
kernfs on /kern type kernfs (local)
ptyfs on /dev/pts type ptyfs (local)
procfs on /proc type procfs (local)


df (disk space)
---------------
Filesystem    1K-blocks       Used      Avail %Cap Mounted on
/dev/xbd1a       981815     660963     271762  70% /
/dev/xbd0a       251823       2211     236505   0% /grub
kernfs                1          1          0 100% /kern
ptyfs                 1          1          0 100% /dev/pts
procfs                4          4          0 100% /proc


Messing with the FS
===================

Filling Disk
------------
To fill the entire disk, I ran "dd if=/dev/zero of=nulls bs=$((1024*1024))",
since reading from /dev/zero returns the null character and we don't specify a
count parameter, so it just continuous reads until we run out of disk space.
Also, dd(1) didn't understand the "M" suffix for Mebibytes (I think it's not 
POSIX), so 1024*1024 sufficed. After running for about 20 seconds, the disk
was full:

/: write failed, file system is full
dd: nulls: No space left on device
313+0 records in
312+0 records out
327155712 bytes transferred in 19.653 secs (16646604 bytes/sec)

Running "df -h":
Filesystem         Size       Used      Avail %Cap Mounted on
/dev/xbd1a         959M       958M       -47M 105% /
/dev/xbd0a         246M       2.2M       231M   0% /grub
kernfs             1.0K       1.0K         0B 100% /kern
ptyfs              1.0K       1.0K         0B 100% /dev/pts
procfs             4.0K       4.0K         0B 100% /proc

After filling the disk, the most infuriating aspect was not having my extensive
shell command history. Interestingly, I was still able to kill my session and 
ssh back into the machine, I was expecting to get a refused connection. Also, I
was able to write write to some files ("echo helloworld > test.txt"), so I'm 
not sure if something odd is happening behind the scenes.

Using All inodes
----------------
After cleaning up the large files, I figured the fastest way to use up all the
inodes was to create symlinks to the same variable:
	"NUM_NODES=$(df -i | awk 'FNR == 2 {print $7}'); \
		for i in $(seq 1 $NUM_NODES); do ln -s many_syms link-$i; done"
I wasn't sure if there was a better way, but after this took awhile to run, all
the inodes on the system were used. I had no problems attaching my ssh session
after, but I was not able to make any additional file on my system, which is 
quite obvious.

Takeaways
=========
Not sure what was wrong with fdisk, although I am neither and expert with 
filesystems nor NetBSD. Also, I've had disks fill up on machines in the past
(due to pesky logs) and that has caused my ssh access to be refused, so I'm not
sure what is allowing my access after the disk is full.

